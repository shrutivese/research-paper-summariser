Title: Exploring FPGA designs for MX and beyond
Authors: Ebby Samson, Naveen Mellempudi, Wayne Luk, George A. Constantinides
Published: 2024-07-01T17:07:33Z
Link: http://arxiv.org/abs/2407.01475v1

Summary:
A number of companies recently worked together to release the new Open
Compute Project MX standard for low-precision computation, aimed at efficient
neural network implementation. In this paper, we describe and evaluate the
first open-source FPGA implementation of the arithmetic defined in the
standard. Our designs fully support all the standard's concrete formats for
conversion into and out of MX formats and for the standard-defined arithmetic
operations, as well as arbitrary fixed-point and floating-point formats.
Certain elements of the standard are left as implementation-defined, and we
present the first concrete FPGA-inspired choices for these elements, which we
outline in the paper. Our library of optimized hardware components is available
open source, and can be used to build larger systems. For this purpose, we also
describe and release an open-source Pytorch library for quantization into the
new standard, integrated with the Brevitas library so that the community can
develop novel neural network designs quantized with MX formats in mind. We
demonstrate the usability and efficacy of our libraries via the implementation
of example neural networks such as ResNet-18 on the ImageNet ILSVRC12 dataset.
Our testing shows that MX is very effective for formats such as INT5 or FP6
which are not natively supported on GPUs. This gives FPGAs an advantage as they
have the flexibility to implement a custom datapath and take advantage of the
smaller area footprints offered by these formats.
